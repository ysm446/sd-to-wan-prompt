"""
Gradio UIå®Ÿè£…
"""
import gradio as gr
import json
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple

from src.core.image_parser import ImageParser
from src.core.model_manager import ModelManager
from src.core.vlm_interface import VLMInterface
from src.utils.config_loader import ConfigLoader


class PromptAnalyzerUI:
    """ãƒ¡ã‚¤ãƒ³UIã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Dict):
        """
        Args:
            config: settings.yamlã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®š
        """
        self.config = config
        self.model_manager = ModelManager(config['paths']['models_dir'])
        self.current_vlm: Optional[VLMInterface] = None
        self.current_image_path: Optional[str] = None
        self.current_metadata: Optional[Dict] = None
        self.selected_model_path: Optional[str] = None  # é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        self.settings_cache_file = Path("settings_cache.json")

        # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        config_loader = ConfigLoader()
        self.model_presets = config_loader.load_model_presets()

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        self.device_settings = self.load_device_settings()

    def create_interface(self) -> gr.Blocks:
        """
        Gradio UIã‚’æ§‹ç¯‰

        UIæ§‹æˆ:
        - ã‚¿ãƒ–1: ç”»åƒåˆ†æ
        - ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
        - ã‚¿ãƒ–3: è¨­å®š
        """
        # ã‚«ã‚¹ã‚¿ãƒ CSSï¼ˆãƒ•ã‚©ãƒ³ãƒˆå¤‰æ›´ï¼‰
        # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒªã‚¢ã®ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ï¼æ‹¡å¤§ã‚¢ã‚¤ã‚³ãƒ³ã‚’éè¡¨ç¤ºã«ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ 
        custom_css = """
        * {
            font-family: "Segoe UI", "Yu Gothic", "Meiryo", Arial, sans-serif !important;
        }
        /* Hide Gradio image fullscreen/expand icons by common attributes and keywords (case-insensitive) */
        button[aria-label*="full" i],
        button[title*="full" i],
        button[aria-label*="expand" i],
        button[title*="expand" i],
        button[aria-label*="fullscreen" i],
        button[title*="fullscreen" i] {
            display: none !important;
        }
        /* Additional selectors for possible Gradio class names */
        .gr-image .gr-button, .gr-image .gr-button--icon, .gr-image__open-fullscreen, .gr-image__expand {
            display: none !important;
        }
        """

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°configã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
        cached_settings = self.load_inference_settings()
        initial_temperature = cached_settings.get('temperature', self.config['inference']['temperature'])
        initial_max_tokens = cached_settings.get('max_tokens', self.config['inference']['max_tokens'])
        initial_top_p = cached_settings.get('top_p', self.config['inference']['top_p'])

        with gr.Blocks(title="WAN Prompt Generator", css=custom_css) as interface:
            gr.Markdown("# WAN Prompt Generator <small style='font-size:0.4em; color:gray;'>SDç”»åƒã‹ã‚‰WAN 2.2ç”¨ã®å‹•ç”»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ</small>")

            with gr.Tabs():
                # ã‚¿ãƒ–1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
                with gr.Tab("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"):
                    with gr.Row():
                        # å·¦å´: ç”»åƒè¡¨ç¤º
                        with gr.Column(scale=1):
                            image_display = gr.Image(
                                label="ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                                type="filepath",
                                sources=["upload"],
                                height=400
                            )
                            image_filename_display = gr.Markdown(
                                value="<small style='color: gray;'>--</small>"
                            )

                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º
                            with gr.Accordion("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±", open=True):
                                prompt_display = gr.Textbox(
                                    label="Prompt",
                                    lines=3,
                                    interactive=False
                                )
                                negative_prompt_display = gr.Textbox(
                                    label="Negative Prompt",
                                    lines=2,
                                    interactive=False
                                )
                                settings_display = gr.Code(
                                    label="Settings",
                                    language="json",
                                    interactive=False,
                                    lines=5
                                )

                        # å³å´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
                        with gr.Column(scale=1):
                            # ç”Ÿæˆçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
                            output_textbox = gr.Textbox(
                                label="ç”Ÿæˆã•ã‚ŒãŸWANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                                lines=18,
                                max_lines=25,
                                interactive=True  # ã‚³ãƒ”ãƒ¼ã§ãã‚‹ã‚ˆã†ã«interactiveã«
                            )
                            with gr.Row():
                                context_info = gr.Markdown(
                                    value="<small style='color: gray;'>--</small>",
                                    elem_id="context-info"
                                )
                                mini_unload_btn = gr.Button(
                                    value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰",
                                    variant="secondary",
                                    size="sm",
                                    interactive=False,
                                    min_width=60,
                                    scale=0
                                )

                            # è¿½åŠ æŒ‡ç¤ºå…¥åŠ›æ¬„
                            additional_input = gr.Textbox(
                                label="è¿½åŠ æŒ‡ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
                                placeholder="ä¾‹: ã‚«ãƒ¡ãƒ©ã‚’ã‚ºãƒ¼ãƒ ã‚¢ã‚¦ãƒˆã•ã›ã¦ã€é«ªã‚’ãªã³ã‹ã›ã¦ãã ã•ã„",
                                lines=2
                            )

                            generate_btn = gr.Button("WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ", variant="primary", size="lg")

                            # å‡ºåŠ›è¨€èªãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠï¼ˆæ¨ªä¸€åˆ—ï¼‰
                            cached_settings = self.load_generation_settings()
                            with gr.Row():
                                language_dropdown = gr.Dropdown(
                                    label="å‡ºåŠ›è¨€èª",
                                    choices=["English", "æ—¥æœ¬èª"],
                                    value=cached_settings.get("language", "English"),
                                    interactive=True,
                                    scale=1
                                )
                                style_dropdown = gr.Dropdown(
                                    label="ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                                    choices=[
                                        ("ãªã—", None),
                                        ("ç©ã‚„ã‹", "calm"),
                                        ("ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯", "dynamic"),
                                        ("ã‚·ãƒãƒãƒ†ã‚£ãƒƒã‚¯", "cinematic"),
                                        ("ã‚¢ãƒ‹ãƒ¡é¢¨", "anime")
                                    ],
                                    value=cached_settings.get("style_preset"),
                                    interactive=True,
                                    scale=1
                                )

                            # å‡ºåŠ›é …ç›®é¸æŠ
                            cached_sections = self.load_output_sections()
                            output_sections = gr.CheckboxGroup(
                                label="å‡ºåŠ›é …ç›®",
                                choices=[
                                    ("ã‚·ãƒ¼ãƒ³", "scene"),
                                    ("ã‚¢ã‚¯ã‚·ãƒ§ãƒ³", "action"),
                                    ("ã‚«ãƒ¡ãƒ©", "camera"),
                                    ("ã‚¹ã‚¿ã‚¤ãƒ«", "style"),
                                    ("WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "prompt")
                                ],
                                value=cached_sections,
                                interactive=True
                            )

                            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
                            with gr.Accordion("ãƒ¢ãƒ‡ãƒ«è¨­å®š", open=False):
                                model_dropdown = gr.Dropdown(
                                    label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
                                    choices=[],
                                    value=None,
                                    interactive=True
                                )

                                # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
                                with gr.Row():
                                    device_dropdown = gr.Dropdown(
                                        label="è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹",
                                        choices=self._get_available_devices(),
                                        value=self.device_settings['device'],
                                        interactive=True,
                                        scale=1,
                                        info="ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹"
                                    )
                                    dtype_dropdown = gr.Dropdown(
                                        label="ãƒ‡ãƒ¼ã‚¿å‹",
                                        choices=[
                                            ("BFloat16ï¼ˆæ¨å¥¨ãƒ»é«˜é€Ÿï¼‰", "bfloat16"),
                                            ("Float16ï¼ˆGPUé«˜é€Ÿï¼‰", "float16"),
                                            ("Float32ï¼ˆé«˜ç²¾åº¦ãƒ»ä½é€Ÿï¼‰", "float32")
                                        ],
                                        value=self.device_settings['dtype'],
                                        interactive=True,
                                        scale=1
                                    )

                                device_info = gr.Markdown(value=self._get_device_info())

                                with gr.Row():
                                    load_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
                                    unload_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢")
                                model_status = gr.Textbox(
                                    label="ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹",
                                    value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰",
                                    interactive=False
                                )

                                # è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®š
                                cached_auto_unload = self.load_auto_unload_setting()
                                auto_unload_checkbox = gr.Checkbox(
                                    label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå¾Œã«è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    value=cached_auto_unload,
                                    info="WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†å¾Œã€è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦VRAMã‚’è§£æ”¾ã—ã¾ã™",
                                    interactive=True
                                )

                            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                            cached_save_dir = self.load_save_directory()
                            with gr.Accordion("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜", open=False):
                                save_dir_input = gr.Textbox(
                                    label="ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€",
                                    placeholder="ä¾‹: D:\\images\\output",
                                    value=cached_save_dir,
                                    lines=1
                                )
                                save_btn = gr.Button(
                                    "ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜", variant="primary"
                                )
                                save_status = gr.Textbox(
                                    label="ä¿å­˜çµæœ",
                                    interactive=False
                                )

                # ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
                with gr.Tab("ãƒ¢ãƒ‡ãƒ«ç®¡ç†"):
                    gr.Markdown("### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«")
                    refresh_models_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°")
                    local_models_display = gr.DataFrame(
                        headers=["ãƒ¢ãƒ‡ãƒ«å", "ãƒ‘ã‚¹", "ã‚µã‚¤ã‚º"],
                        datatype=["str", "str", "str"],
                        label="ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«"
                    )

                    gr.Markdown("### ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    with gr.Row():
                        with gr.Column():
                            preset_dropdown = gr.Dropdown(
                                label="ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                                choices=list(self.model_presets.keys()),
                                value=None
                            )
                            repo_id_input = gr.Textbox(
                                label="Repository ID",
                                placeholder="Qwen/Qwen2-VL-7B-Instruct",
                                value=""
                            )
                            local_name_input = gr.Textbox(
                                label="ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å",
                                placeholder="qwen2-vl-7b",
                                value=""
                            )
                            download_btn = gr.Button("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹", variant="primary")

                        with gr.Column():
                            preset_info = gr.Markdown("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
                            download_status = gr.Textbox(
                                label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹",
                                value="",
                                interactive=False,
                                lines=5
                            )

                # ã‚¿ãƒ–3: è¨­å®š
                with gr.Tab("è¨­å®š"):
                    with gr.Row():
                        with gr.Column():
                            temperature_slider = gr.Slider(
                                label="Temperature",
                                info="ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ï¼ˆä½ã„å€¤=æ­£ç¢ºã€é«˜ã„å€¤=å‰µé€ çš„ï¼‰ã€‚ç”»åƒåˆ†æã§ã¯0.1ï½0.3ã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=2.0,
                                value=initial_temperature,
                                step=0.1
                            )
                            max_tokens_slider = gr.Slider(
                                label="Max Tokens",
                                info="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ–‡ç« ã®é•·ã•ï¼‰",
                                minimum=64,
                                maximum=2048,
                                value=initial_max_tokens,
                                step=64
                            )
                            top_p_slider = gr.Slider(
                                label="Top P",
                                info="èªå½™ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã€‚0.9å‰å¾Œã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=1.0,
                                value=initial_top_p,
                                step=0.05
                            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆupload/clearã‚¤ãƒ™ãƒ³ãƒˆã§åˆ†é›¢ã—ã¦å‡¦ç†ï¼‰
            image_display.upload(
                fn=self.on_image_upload,
                inputs=[image_display],
                outputs=[image_filename_display, prompt_display, negative_prompt_display, settings_display]
            )
            image_display.clear(
                fn=self.on_image_clear,
                inputs=[],
                outputs=[image_filename_display, prompt_display, negative_prompt_display, settings_display]
            )

            # WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            generate_btn.click(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, style_dropdown, language_dropdown, output_sections, temperature_slider, max_tokens_slider, auto_unload_checkbox],
                outputs=[output_textbox, context_info, model_status, mini_unload_btn]
            )

            # è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            auto_unload_checkbox.change(
                fn=self.save_auto_unload_setting,
                inputs=[auto_unload_checkbox],
                outputs=[]
            )

            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            save_btn.click(
                fn=self.save_prompt_to_file,
                inputs=[save_dir_input, output_textbox, additional_input],
                outputs=[save_status]
            )

            # ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            save_dir_input.change(
                fn=self.save_save_directory,
                inputs=[save_dir_input],
                outputs=[]
            )

            # å‡ºåŠ›é …ç›®ã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            output_sections.change(
                fn=self.save_output_sections,
                inputs=[output_sections],
                outputs=[]
            )

            # è¨€èªãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            language_dropdown.change(
                fn=lambda lang, style: self.save_generation_settings(lang, style),
                inputs=[language_dropdown, style_dropdown],
                outputs=[]
            )
            style_dropdown.change(
                fn=lambda lang, style: self.save_generation_settings(lang, style),
                inputs=[language_dropdown, style_dropdown],
                outputs=[]
            )

            # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
            refresh_models_btn.click(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

            # ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®å¤‰æ›´æ™‚ã«é¸æŠã‚’ä¿å­˜
            def save_selected_model(path):
                self.selected_model_path = path
                self.save_last_model_path(path) if path else None

            model_dropdown.change(
                fn=save_selected_model,
                inputs=[model_dropdown],
                outputs=[]
            )

            load_model_btn.click(
                fn=self.load_vlm_model_with_btn,
                inputs=[model_dropdown],
                outputs=[model_status, context_info, mini_unload_btn]
            )

            unload_model_btn.click(
                fn=self.unload_vlm_model_with_btn,
                outputs=[model_status, context_info, mini_unload_btn]
            )

            # ãƒ‡ãƒã‚¤ã‚¹å¤‰æ›´æ™‚ã®ã‚¤ãƒ™ãƒ³ãƒˆ
            device_dropdown.change(
                fn=self.on_device_change,
                inputs=[device_dropdown, dtype_dropdown],
                outputs=[model_status, context_info, mini_unload_btn, device_info]
            )

            dtype_dropdown.change(
                fn=self.on_device_change,
                inputs=[device_dropdown, dtype_dropdown],
                outputs=[model_status, context_info, mini_unload_btn, device_info]
            )

            mini_unload_btn.click(
                fn=self.unload_vlm_model_with_btn,
                outputs=[model_status, context_info, mini_unload_btn]
            )

            preset_dropdown.change(
                fn=self.update_preset_info,
                inputs=[preset_dropdown],
                outputs=[preset_info, repo_id_input, local_name_input]
            )

            download_btn.click(
                fn=self.download_model,
                inputs=[repo_id_input, local_name_input],
                outputs=[download_status]
            )

            # æ¨è«–è¨­å®šã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
            def on_settings_change(temp, tokens, top_p):
                self.save_inference_settings(temp, tokens, top_p)

            temperature_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            max_tokens_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            top_p_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )

            # åˆæœŸãƒ­ãƒ¼ãƒ‰
            interface.load(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

        return interface

    def on_image_clear(self) -> Tuple:
        """ç”»åƒãŒã‚¯ãƒªã‚¢ã•ã‚ŒãŸã¨ãã®å‡¦ç†"""
        self.current_image_path = None
        self.current_metadata = None
        return "<small style='color: gray;'>--</small>", "", "", "{}"

    def on_image_upload(self, image_path: str) -> Tuple:
        """ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã¨ãã®å‡¦ç†"""
        try:
            # ç”»åƒãƒ‘ã‚¹ãŒNoneã¾ãŸã¯ç©ºã®å ´åˆã¯ã‚¯ãƒªã‚¢
            if not image_path:
                return self.on_image_clear()

            # ç”»åƒãƒ‘ã‚¹ã‚’ä¿å­˜
            self.current_image_path = image_path

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            self.current_metadata = ImageParser.extract_metadata(image_path)

            # Settingsã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
            settings_json = json.dumps(self.current_metadata['settings'], indent=2, ensure_ascii=False)

            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º
            filename = Path(image_path).name
            filename_md = f"<small>ğŸ“ {filename}</small>"

            return (
                filename_md,
                self.current_metadata['prompt'],
                self.current_metadata['negative_prompt'],
                settings_json
            )
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            self.current_image_path = None
            self.current_metadata = None
            return "<small style='color: gray;'>--</small>", "ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚", "", "{}"

    def _get_unload_btn_update(self):
        """ãƒŸãƒ‹ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤ºçŠ¶æ…‹ã‚’å–å¾—"""
        if self.current_vlm is not None:
            return gr.update(
                value="â ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰", variant="stop", interactive=True
            )
        else:
            return gr.update(
                value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰", variant="secondary", interactive=False
            )

    def load_vlm_model_with_btn(
        self, model_path: str
    ) -> Tuple[str, str, dict]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒŸãƒ‹ãƒœã‚¿ãƒ³æ›´æ–°ä»˜ãï¼‰"""
        status, context = self.load_vlm_model(model_path)
        return status, context, self._get_unload_btn_update()

    def unload_vlm_model_with_btn(self) -> Tuple[str, str, dict]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒŸãƒ‹ãƒœã‚¿ãƒ³æ›´æ–°ä»˜ãï¼‰"""
        status, context = self.unload_vlm_model()
        return status, context, self._get_unload_btn_update()

    def _get_model_status(self) -> str:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’å–å¾—"""
        if self.current_vlm is None:
            return "ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰"
        if self.selected_model_path:
            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {Path(self.selected_model_path).name}"
        return "ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿"

    def generate_wan_prompt(
        self,
        additional_instruction: str,
        style_preset: str,
        output_language: str,
        output_sections: List[str],
        temperature: float,
        max_tokens: int,
        auto_unload: bool
    ):
        """WAN 2.2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        max_tokens_int = int(max_tokens)
        # å‡ºåŠ›é …ç›®ãŒç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if not output_sections:
            output_sections = ["scene", "action", "camera", "style", "prompt"]

        # ãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
        if self.current_vlm is None and self.selected_model_path:
            yield "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...", "<small style='color: gray;'>ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...</small>", "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...", gr.update()

            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            status, context = self.load_vlm_model(self.selected_model_path)

            if "âœ“" not in status:
                yield f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n{status}", "<small style='color: gray;'>--</small>", status, self._get_unload_btn_update()
                return

        if self.current_vlm is None:
            yield "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", "<small style='color: gray;'>--</small>", "ãƒ¢ãƒ‡ãƒ«æœªé¸æŠ", self._get_unload_btn_update()
            return

        if not self.current_image_path or self.current_metadata is None:
            yield "ã‚¨ãƒ©ãƒ¼: ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", self._get_context_info_simple(), self._get_model_status(), self._get_unload_btn_update()
            return

        prompt_text = self.current_metadata['prompt']

        try:
            # VLMã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
            response = ""
            start_time = time.time()
            for chunk in self.current_vlm.generate_wan_prompt_stream(
                image_path=self.current_image_path,
                sd_prompt=prompt_text,
                additional_instruction=additional_instruction or "",
                style_preset=style_preset,
                output_language=output_language or "English",
                output_sections=output_sections,
                temperature=temperature,
                max_tokens=max_tokens_int
            ):
                response += chunk
                yield response, self._get_context_info_simple(), self._get_model_status(), self._get_unload_btn_update()

            # ç”Ÿæˆæ™‚é–“ã‚’è¡¨ç¤º
            elapsed_time = time.time() - start_time
            context_with_time = f"<small style='color: gray;'>ç”Ÿæˆå®Œäº† ({elapsed_time:.1f}ç§’)</small>"
            yield response, context_with_time, self._get_model_status(), self._get_unload_btn_update()

            # è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if auto_unload:
                self.unload_vlm_model()
                yield response, "<small style='color: gray;'>âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ</small>", "âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼ˆVRAMã‚’è§£æ”¾ï¼‰", self._get_unload_btn_update()

        except Exception as e:
            yield f"ã‚¨ãƒ©ãƒ¼: {str(e)}", self._get_context_info_simple(), self._get_model_status(), self._get_unload_btn_update()

    def _get_context_info_simple(self) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
        if self.current_vlm is None:
            return "<small style='color: gray;'>--</small>"

        context_length = self.current_vlm.get_context_length()
        if context_length > 0:
            return f"<small style='color: gray;'>ğŸ“Š ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {context_length:,}</small>"
        return "<small style='color: gray;'>--</small>"

    def _get_device_info(self) -> str:
        """ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—"""
        import torch

        info_lines = []

        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            info_lines.append(f"âœ“ **CUDAåˆ©ç”¨å¯èƒ½**")
            info_lines.append(f"  - GPU: {gpu_name}")
            info_lines.append(f"  - VRAM: {gpu_memory:.1f} GB")
        else:
            info_lines.append("âš  **CUDAåˆ©ç”¨ä¸å¯** - CPUãƒ¢ãƒ¼ãƒ‰ã®ã¿")

        return "<small>" + "<br>".join(info_lines) + "</small>"

    def _get_available_devices(self) -> List[Tuple[str, str]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹é¸æŠè‚¢ã‚’å–å¾—"""
        import torch

        choices = [
            ("è‡ªå‹•é¸æŠï¼ˆGPUå„ªå…ˆï¼‰", "auto"),
            ("CPU", "cpu")
        ]

        if torch.cuda.is_available():
            choices.append(("CUDAï¼ˆGPUï¼‰", "cuda"))

        return choices

    def refresh_local_models(self) -> Tuple:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°"""
        models = self.model_manager.list_local_models()

        # DataFrameãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        df_data = [[m['name'], m['path'], m['size']] for m in models]

        # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ç”¨ã®é¸æŠè‚¢
        choices = [m['path'] for m in models]

        # å‰å›ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        last_model_path = self.load_last_model_path()

        # å‰å›ã®ãƒ¢ãƒ‡ãƒ«ãŒã¾ã å­˜åœ¨ã™ã‚‹å ´åˆã¯åˆæœŸå€¤ã«è¨­å®š
        if last_model_path and last_model_path in choices:
            self.selected_model_path = last_model_path
            # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®valueã‚’è¨­å®šã—ã¦UIã«è¡¨ç¤º
            return df_data, gr.update(choices=choices, value=last_model_path)

        # å‰å›ã®ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯é¸æŠçŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
        self.selected_model_path = None
        return df_data, gr.update(choices=choices, value=None)

    def load_vlm_model(self, model_path: str) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if not model_path:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", "<small style='color: gray;'>--</small>"

        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜
        self.selected_model_path = model_path

        try:
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if self.current_vlm is not None:
                self.current_vlm.unload_model()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’èª­ã¿è¾¼ã¿
            device_settings = self.load_device_settings()

            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.current_vlm = VLMInterface(
                model_path=model_path,
                device=device_settings['device'],
                dtype=device_settings['dtype']
            )

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’å–å¾—
            context_length = self.current_vlm.get_context_length()
            if context_length > 0:
                context_info = f"<small style='color: gray;'>ğŸ“Š CONTEXT: 0 / {context_length:,}</small>"
            else:
                context_info = "<small style='color: gray;'>ğŸ“Š CONTEXT: 0</small>"

            # æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜
            self.save_last_model_path(model_path)

            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {Path(model_path).name}", context_info

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}", "<small style='color: gray;'>--</small>"

    def unload_vlm_model(self) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦VRAMã‚’è§£æ”¾"""
        if self.current_vlm is None:
            return "ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™", "<small style='color: gray;'>--</small>"

        try:
            self.current_vlm.unload_model()
            self.current_vlm = None
            return "âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼ˆVRAMã‚’è§£æ”¾ï¼‰", "<small style='color: gray;'>--</small>"
        except Exception as e:
            return f"âœ— ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {str(e)}", "<small style='color: gray;'>--</small>"

    def on_device_change(
        self,
        device: str,
        dtype: str
    ) -> Tuple[str, str, dict, str]:
        """
        ãƒ‡ãƒã‚¤ã‚¹/ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›´æ™‚ã®å‡¦ç†

        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. è¨­å®šã‚’ settings_cache.json ã«ä¿å­˜
        2. ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆ:
           - æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
           - æ–°ã—ã„ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’å†ãƒ­ãƒ¼ãƒ‰
           - UIã‚’æ›´æ–°
        3. ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰ã®å ´åˆ:
           - è¨­å®šã®ã¿ä¿å­˜ï¼ˆæ¬¡å›ãƒ­ãƒ¼ãƒ‰æ™‚ã«åæ˜ ï¼‰

        Args:
            device: æ–°ã—ã„ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
            dtype: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å‹è¨­å®š

        Returns:
            (model_status, context_info, mini_unload_btn_update, device_info)
        """
        # è¨­å®šã‚’ä¿å­˜
        self.save_device_settings(device, dtype)

        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’æ›´æ–°
        device_info_text = self._get_device_info()

        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆã¯å†ãƒ­ãƒ¼ãƒ‰
        if self.current_vlm is not None and self.selected_model_path:
            try:
                # ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
                self.current_vlm.unload_model()
                self.current_vlm = None

                # æ–°ã—ã„ãƒ‡ãƒã‚¤ã‚¹ã§ãƒ­ãƒ¼ãƒ‰
                self.current_vlm = VLMInterface(
                    model_path=self.selected_model_path,
                    device=device,
                    dtype=dtype
                )

                context_length = self.current_vlm.get_context_length()
                context_info = f"<small style='color: gray;'>ğŸ“Š CONTEXT: 0 / {context_length:,}</small>"

                device_name = {"auto": "è‡ªå‹•é¸æŠ", "cpu": "CPU", "cuda": "CUDAï¼ˆGPUï¼‰"}.get(device, device)
                return (
                    f"âœ“ ãƒ‡ãƒã‚¤ã‚¹ã‚’å¤‰æ›´ã—ã¾ã—ãŸ: {device_name}\nãƒ¢ãƒ‡ãƒ«: {Path(self.selected_model_path).name}",
                    context_info,
                    self._get_unload_btn_update(),
                    device_info_text
                )
            except Exception as e:
                return (
                    f"âœ— ãƒ‡ãƒã‚¤ã‚¹å¤‰æ›´ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    "<small style='color: gray;'>--</small>",
                    self._get_unload_btn_update(),
                    device_info_text
                )

        # ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰æ™‚
        return (
            "ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆæ¬¡å›ãƒ­ãƒ¼ãƒ‰æ™‚ã«åæ˜ ï¼‰",
            "<small style='color: gray;'>--</small>",
            self._get_unload_btn_update(),
            device_info_text
        )

    def update_preset_info(self, preset_name: str) -> Tuple:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if not preset_name or preset_name not in self.model_presets:
            return "ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™", "", ""

        preset = self.model_presets[preset_name]

        info_md = f"""
### {preset_name}

**èª¬æ˜**: {preset['description']}
**æ¨å¥¨ç”¨é€”**: {preset['recommended_for']}
**Repository ID**: `{preset['repo_id']}`
"""

        return info_md, preset['repo_id'], preset['local_name']

    def save_last_model_path(self, model_path: str):
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆsettingså«ã‚€ï¼‰"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æ›´æ–°
            data["last_model"] = model_path

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def save_inference_settings(self, temperature: float, max_tokens: int, top_p: float):
        """æ¨è«–è¨­å®šã‚’ä¿å­˜"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # è¨­å®šã‚’æ›´æ–°
            data["inference_settings"] = {
                "temperature": temperature,
                "max_tokens": int(max_tokens),
                "top_p": top_p
            }

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_last_model_path(self) -> Optional[str]:
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("last_model")
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

    def load_inference_settings(self) -> dict:
        """æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("inference_settings", {})
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

    def save_output_sections(self, sections: List[str]):
        """å‡ºåŠ›é …ç›®ã®é¸æŠçŠ¶æ…‹ã‚’ä¿å­˜"""
        try:
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            data["output_sections"] = sections

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: å‡ºåŠ›é …ç›®ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_output_sections(self) -> List[str]:
        """å‡ºåŠ›é …ç›®ã®é¸æŠçŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
        default_sections = ["scene", "action", "camera", "style", "prompt"]
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("output_sections", default_sections)
        except Exception as e:
            print(f"è­¦å‘Š: å‡ºåŠ›é …ç›®ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return default_sections

    def save_generation_settings(self, language: str, style_preset: str):
        """è¨€èªãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã®é¸æŠçŠ¶æ…‹ã‚’ä¿å­˜"""
        try:
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            data["generation_settings"] = {
                "language": language,
                "style_preset": style_preset
            }

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_generation_settings(self) -> dict:
        """è¨€èªãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã®é¸æŠçŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("generation_settings", {})
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆè¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

    def save_save_directory(self, save_dir: str):
        """ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            data["save_directory"] = save_dir

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_save_directory(self) -> str:
        """ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("save_directory", "")
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return ""

    def save_device_settings(self, device: str, dtype: str):
        """ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ settings_cache.json ã«ä¿å­˜"""
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’æ›´æ–°
            data["device_settings"] = {
                "device": device,
                "dtype": dtype
            }

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_device_settings(self) -> Dict[str, str]:
        """ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ settings_cache.json ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆsettings.yaml ã‹ã‚‰å–å¾—ï¼‰
        default_settings = {
            "device": self.config.get('model', {}).get('device', 'auto'),
            "dtype": self.config.get('model', {}).get('dtype', 'bfloat16')
        }

        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("device_settings", default_settings)
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        return default_settings

    def save_auto_unload_setting(self, auto_unload: bool):
        """è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            data = {}
            if self.settings_cache_file.exists():
                try:
                    data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            data["auto_unload"] = auto_unload

            self.settings_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_auto_unload_setting(self) -> bool:
        """è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            if self.settings_cache_file.exists():
                data = json.loads(self.settings_cache_file.read_text(encoding='utf-8'))
                return data.get("auto_unload", False)
        except Exception as e:
            print(f"è­¦å‘Š: è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False

    def save_prompt_to_file(
        self,
        save_dir: str,
        output_text: str,
        additional_instruction: str
    ) -> str:
        """å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»è¿½åŠ æŒ‡ç¤ºãƒ»ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.current_image_path:
            return "ã‚¨ãƒ©ãƒ¼: ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"

        if not save_dir or not save_dir.strip():
            return "ã‚¨ãƒ©ãƒ¼: ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"

        if not output_text or not output_text.strip():
            return "ã‚¨ãƒ©ãƒ¼: ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚Šã¾ã›ã‚“"

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ .txt ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ
        image_filename = Path(self.current_image_path).stem + ".txt"
        save_path = Path(save_dir.strip()) / image_filename

        # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’çµ„ã¿ç«‹ã¦
        lines: list[str] = []

        # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        original_prompt = ""
        if self.current_metadata:
            original_prompt = self.current_metadata.get('prompt', '')
        if original_prompt:
            lines.append("=== Original Prompt ===")
            lines.append(original_prompt)
            lines.append("")

        # è¿½åŠ æŒ‡ç¤º
        if additional_instruction and additional_instruction.strip():
            lines.append("=== Additional Instruction ===")
            lines.append(additional_instruction.strip())
            lines.append("")

        # ç”Ÿæˆã•ã‚ŒãŸWANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        lines.append("=== Generated WAN Prompt ===")
        lines.append(output_text.strip())
        lines.append("")

        try:
            save_path.write_text("\n".join(lines), encoding='utf-8')
            return f"âœ“ ä¿å­˜ã—ã¾ã—ãŸ: {save_path}"
        except Exception as e:
            return f"âœ— ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"

    def download_model(self, repo_id: str, local_name: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if not repo_id:
            return "ã‚¨ãƒ©ãƒ¼: Repository IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

        try:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            downloaded_path = self.model_manager.download_model(
                repo_id=repo_id,
                local_name=local_name if local_name else None
            )

            return f"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\nã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}"
