"""
Gradio UIå®Ÿè£…
"""
import gradio as gr
import json
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple

from src.core.image_parser import ImageParser
from src.core.model_manager import ModelManager
from src.core.vlm_interface import VLMInterface
from src.utils.config_loader import ConfigLoader


class PromptAnalyzerUI:
    """ãƒ¡ã‚¤ãƒ³UIã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Dict):
        """
        Args:
            config: settings.yamlã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®š
        """
        self.config = config
        self.model_manager = ModelManager(config['paths']['models_dir'])
        self.current_vlm: Optional[VLMInterface] = None
        self.current_image_path: Optional[str] = None
        self.current_metadata: Optional[Dict] = None
        self.selected_model_path: Optional[str] = None  # é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        self.last_model_cache_file = Path(".last_model_cache.json")

        # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        config_loader = ConfigLoader()
        self.model_presets = config_loader.load_model_presets()

    def create_interface(self) -> gr.Blocks:
        """
        Gradio UIã‚’æ§‹ç¯‰

        UIæ§‹æˆ:
        - ã‚¿ãƒ–1: ç”»åƒåˆ†æ
        - ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
        - ã‚¿ãƒ–3: è¨­å®š
        """
        # ã‚«ã‚¹ã‚¿ãƒ CSSï¼ˆãƒ•ã‚©ãƒ³ãƒˆå¤‰æ›´ï¼‰
        custom_css = """
        * {
            font-family: "Segoe UI", "Yu Gothic", "Meiryo", Arial, sans-serif !important;
        }
        """

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°configã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
        cached_settings = self.load_inference_settings()
        initial_temperature = cached_settings.get('temperature', self.config['inference']['temperature'])
        initial_max_tokens = cached_settings.get('max_tokens', self.config['inference']['max_tokens'])
        initial_top_p = cached_settings.get('top_p', self.config['inference']['top_p'])

        with gr.Blocks(title="WAN Prompt Generator", css=custom_css) as interface:
            gr.Markdown("# WAN Prompt Generator")
            gr.Markdown("SDç”»åƒã‹ã‚‰WAN 2.2ç”¨ã®å‹•ç”»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™")

            with gr.Tabs():
                # ã‚¿ãƒ–1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
                with gr.Tab("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"):
                    with gr.Row():
                        # å·¦å´: ç”»åƒè¡¨ç¤º
                        with gr.Column(scale=1):
                            image_display = gr.Image(
                                label="SDç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                                type="filepath",
                                sources=["upload"],
                                height=400
                            )

                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º
                            with gr.Accordion("å…ƒã®SDæƒ…å ±", open=True):
                                prompt_display = gr.Textbox(
                                    label="Prompt",
                                    lines=3,
                                    interactive=False
                                )
                                negative_prompt_display = gr.Textbox(
                                    label="Negative Prompt",
                                    lines=2,
                                    interactive=False
                                )
                                settings_display = gr.Code(
                                    label="Settings",
                                    language="json",
                                    interactive=False,
                                    lines=5
                                )

                        # å³å´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
                        with gr.Column(scale=1):
                            # ç”Ÿæˆçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
                            output_textbox = gr.Textbox(
                                label="ç”Ÿæˆã•ã‚ŒãŸWANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                                lines=18,
                                max_lines=25,
                                interactive=True  # ã‚³ãƒ”ãƒ¼ã§ãã‚‹ã‚ˆã†ã«interactiveã«
                            )
                            context_info = gr.Markdown(
                                value="<small style='color: gray;'>--</small>",
                                elem_id="context-info"
                            )

                            # è¿½åŠ æŒ‡ç¤ºå…¥åŠ›æ¬„
                            additional_input = gr.Textbox(
                                label="è¿½åŠ æŒ‡ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
                                placeholder="ä¾‹: ã‚«ãƒ¡ãƒ©ã‚’ã‚ºãƒ¼ãƒ ã‚¢ã‚¦ãƒˆã•ã›ã¦ã€é«ªã‚’ãªã³ã‹ã›ã¦ãã ã•ã„",
                                lines=2
                            )

                            # ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
                            gr.Markdown("### ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆ")
                            with gr.Row():
                                style_calm = gr.Button("ç©ã‚„ã‹", size="sm")
                                style_dynamic = gr.Button("ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯", size="sm")
                            with gr.Row():
                                style_cinematic = gr.Button("ã‚·ãƒãƒãƒ†ã‚£ãƒƒã‚¯", size="sm")
                                style_anime = gr.Button("ã‚¢ãƒ‹ãƒ¡é¢¨", size="sm")

                            # ç¾åœ¨é¸æŠä¸­ã®ã‚¹ã‚¿ã‚¤ãƒ«
                            current_style = gr.State(value="cinematic")

                            generate_btn = gr.Button("WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ", variant="primary", size="lg")

                            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
                            with gr.Accordion("ãƒ¢ãƒ‡ãƒ«è¨­å®š", open=False):
                                model_dropdown = gr.Dropdown(
                                    label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
                                    choices=[],
                                    value=None,
                                    interactive=True
                                )
                                with gr.Row():
                                    load_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
                                    unload_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢")
                                model_status = gr.Textbox(
                                    label="ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹",
                                    value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰",
                                    interactive=False
                                )

                # ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
                with gr.Tab("ãƒ¢ãƒ‡ãƒ«ç®¡ç†"):
                    gr.Markdown("### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«")
                    refresh_models_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°")
                    local_models_display = gr.DataFrame(
                        headers=["ãƒ¢ãƒ‡ãƒ«å", "ãƒ‘ã‚¹", "ã‚µã‚¤ã‚º"],
                        datatype=["str", "str", "str"],
                        label="ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«"
                    )

                    gr.Markdown("### ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    with gr.Row():
                        with gr.Column():
                            preset_dropdown = gr.Dropdown(
                                label="ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                                choices=list(self.model_presets.keys()),
                                value=None
                            )
                            repo_id_input = gr.Textbox(
                                label="Repository ID",
                                placeholder="Qwen/Qwen2-VL-7B-Instruct",
                                value=""
                            )
                            local_name_input = gr.Textbox(
                                label="ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å",
                                placeholder="qwen2-vl-7b",
                                value=""
                            )
                            download_btn = gr.Button("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹", variant="primary")

                        with gr.Column():
                            preset_info = gr.Markdown("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
                            download_status = gr.Textbox(
                                label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹",
                                value="",
                                interactive=False,
                                lines=5
                            )

                # ã‚¿ãƒ–3: è¨­å®š
                with gr.Tab("è¨­å®š"):
                    with gr.Row():
                        with gr.Column():
                            temperature_slider = gr.Slider(
                                label="Temperature",
                                info="ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ï¼ˆä½ã„å€¤=æ­£ç¢ºã€é«˜ã„å€¤=å‰µé€ çš„ï¼‰ã€‚ç”»åƒåˆ†æã§ã¯0.1ï½0.3ã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=2.0,
                                value=initial_temperature,
                                step=0.1
                            )
                            max_tokens_slider = gr.Slider(
                                label="Max Tokens",
                                info="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ–‡ç« ã®é•·ã•ï¼‰",
                                minimum=64,
                                maximum=2048,
                                value=initial_max_tokens,
                                step=64
                            )
                            top_p_slider = gr.Slider(
                                label="Top P",
                                info="èªå½™ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã€‚0.9å‰å¾Œã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=1.0,
                                value=initial_top_p,
                                step=0.05
                            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆchangeã‚¤ãƒ™ãƒ³ãƒˆã§å‡¦ç†ï¼‰
            image_display.change(
                fn=self.on_image_upload,
                inputs=[image_display],
                outputs=[prompt_display, negative_prompt_display, settings_display]
            )

            # WANãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            generate_btn.click(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, current_style, temperature_slider, max_tokens_slider],
                outputs=[output_textbox, context_info, model_status]
            )

            # ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ - ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨­å®šã—ã¦ç”Ÿæˆ
            style_calm.click(
                fn=lambda: "calm",
                outputs=[current_style]
            ).then(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, current_style, temperature_slider, max_tokens_slider],
                outputs=[output_textbox, context_info, model_status]
            )

            style_dynamic.click(
                fn=lambda: "dynamic",
                outputs=[current_style]
            ).then(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, current_style, temperature_slider, max_tokens_slider],
                outputs=[output_textbox, context_info, model_status]
            )

            style_cinematic.click(
                fn=lambda: "cinematic",
                outputs=[current_style]
            ).then(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, current_style, temperature_slider, max_tokens_slider],
                outputs=[output_textbox, context_info, model_status]
            )

            style_anime.click(
                fn=lambda: "anime",
                outputs=[current_style]
            ).then(
                fn=self.generate_wan_prompt,
                inputs=[additional_input, current_style, temperature_slider, max_tokens_slider],
                outputs=[output_textbox, context_info, model_status]
            )

            # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
            refresh_models_btn.click(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

            # ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®å¤‰æ›´æ™‚ã«é¸æŠã‚’ä¿å­˜
            def save_selected_model(path):
                self.selected_model_path = path
                self.save_last_model_path(path) if path else None

            model_dropdown.change(
                fn=save_selected_model,
                inputs=[model_dropdown],
                outputs=[]
            )

            load_model_btn.click(
                fn=self.load_vlm_model,
                inputs=[model_dropdown],
                outputs=[model_status, context_info]
            )

            unload_model_btn.click(
                fn=self.unload_vlm_model,
                outputs=[model_status, context_info]
            )

            preset_dropdown.change(
                fn=self.update_preset_info,
                inputs=[preset_dropdown],
                outputs=[preset_info, repo_id_input, local_name_input]
            )

            download_btn.click(
                fn=self.download_model,
                inputs=[repo_id_input, local_name_input],
                outputs=[download_status]
            )

            # æ¨è«–è¨­å®šã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
            def on_settings_change(temp, tokens, top_p):
                self.save_inference_settings(temp, tokens, top_p)

            temperature_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            max_tokens_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            top_p_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )

            # åˆæœŸãƒ­ãƒ¼ãƒ‰
            interface.load(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

        return interface

    def on_image_upload(self, image_path: str) -> Tuple:
        """ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã¨ãã®å‡¦ç†"""
        try:
            # ç”»åƒãƒ‘ã‚¹ãŒNoneã¾ãŸã¯ç©ºã®å ´åˆã¯ã‚¯ãƒªã‚¢
            if not image_path:
                self.current_image_path = None
                self.current_metadata = None
                return "", "", "{}"

            # ç”»åƒãƒ‘ã‚¹ã‚’ä¿å­˜
            self.current_image_path = image_path

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            self.current_metadata = ImageParser.extract_metadata(image_path)

            # Settingsã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
            settings_json = json.dumps(self.current_metadata['settings'], indent=2, ensure_ascii=False)

            return (
                self.current_metadata['prompt'],
                self.current_metadata['negative_prompt'],
                settings_json
            )
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            self.current_image_path = None
            self.current_metadata = None
            return "ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚", "", "{}"

    def _get_model_status(self) -> str:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’å–å¾—"""
        if self.current_vlm is None:
            return "ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰"
        if self.selected_model_path:
            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {Path(self.selected_model_path).name}"
        return "ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿"

    def generate_wan_prompt(
        self,
        additional_instruction: str,
        style_preset: str,
        temperature: float,
        max_tokens: int
    ):
        """WAN 2.1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        max_tokens_int = int(max_tokens)

        # ãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
        if self.current_vlm is None and self.selected_model_path:
            yield "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...", "<small style='color: gray;'>ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...</small>", "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."

            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            status, context = self.load_vlm_model(self.selected_model_path)

            if "âœ“" not in status:
                yield f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n{status}", "<small style='color: gray;'>--</small>", status
                return

        if self.current_vlm is None:
            yield "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", "<small style='color: gray;'>--</small>", "ãƒ¢ãƒ‡ãƒ«æœªé¸æŠ"
            return

        if not self.current_image_path or self.current_metadata is None:
            yield "ã‚¨ãƒ©ãƒ¼: ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", self._get_context_info_simple(), self._get_model_status()
            return

        prompt_text = self.current_metadata['prompt']

        try:
            # VLMã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
            response = ""
            start_time = time.time()
            for chunk in self.current_vlm.generate_wan_prompt_stream(
                image_path=self.current_image_path,
                sd_prompt=prompt_text,
                additional_instruction=additional_instruction or "",
                style_preset=style_preset or "cinematic",
                temperature=temperature,
                max_tokens=max_tokens_int
            ):
                response += chunk
                yield response, self._get_context_info_simple(), self._get_model_status()

            # ç”Ÿæˆæ™‚é–“ã‚’è¡¨ç¤º
            elapsed_time = time.time() - start_time
            context_with_time = f"<small style='color: gray;'>ç”Ÿæˆå®Œäº† ({elapsed_time:.1f}ç§’)</small>"
            yield response, context_with_time, self._get_model_status()

        except Exception as e:
            yield f"ã‚¨ãƒ©ãƒ¼: {str(e)}", self._get_context_info_simple(), self._get_model_status()

    def _get_context_info_simple(self) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
        if self.current_vlm is None:
            return "<small style='color: gray;'>--</small>"

        context_length = self.current_vlm.get_context_length()
        if context_length > 0:
            return f"<small style='color: gray;'>ğŸ“Š ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {context_length:,}</small>"
        return "<small style='color: gray;'>--</small>"

    def refresh_local_models(self) -> Tuple:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°"""
        models = self.model_manager.list_local_models()

        # DataFrameãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        df_data = [[m['name'], m['path'], m['size']] for m in models]

        # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ç”¨ã®é¸æŠè‚¢
        choices = [m['path'] for m in models]

        # å‰å›ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        last_model_path = self.load_last_model_path()

        # å‰å›ã®ãƒ¢ãƒ‡ãƒ«ãŒã¾ã å­˜åœ¨ã™ã‚‹å ´åˆã¯åˆæœŸå€¤ã«è¨­å®š
        if last_model_path and last_model_path in choices:
            self.selected_model_path = last_model_path
            return df_data, gr.Dropdown(choices=choices, value=last_model_path)

        return df_data, gr.Dropdown(choices=choices)

    def load_vlm_model(self, model_path: str) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if not model_path:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", "<small style='color: gray;'>--</small>"

        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜
        self.selected_model_path = model_path

        try:
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if self.current_vlm is not None:
                self.current_vlm.unload_model()

            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.current_vlm = VLMInterface(
                model_path=model_path,
                device=self.config['model']['device'],
                dtype=self.config['model']['dtype']
            )

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’å–å¾—
            context_length = self.current_vlm.get_context_length()
            if context_length > 0:
                context_info = f"<small style='color: gray;'>ğŸ“Š CONTEXT: 0 / {context_length:,}</small>"
            else:
                context_info = "<small style='color: gray;'>ğŸ“Š CONTEXT: 0</small>"

            # æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜
            self.save_last_model_path(model_path)

            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {Path(model_path).name}", context_info

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}", "<small style='color: gray;'>--</small>"

    def unload_vlm_model(self) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦VRAMã‚’è§£æ”¾"""
        if self.current_vlm is None:
            return "ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™", "<small style='color: gray;'>--</small>"

        try:
            self.current_vlm.unload_model()
            self.current_vlm = None
            return "âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼ˆVRAMã‚’è§£æ”¾ï¼‰", "<small style='color: gray;'>--</small>"
        except Exception as e:
            return f"âœ— ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {str(e)}", "<small style='color: gray;'>--</small>"

    def update_preset_info(self, preset_name: str) -> Tuple:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if not preset_name or preset_name not in self.model_presets:
            return "ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™", "", ""

        preset = self.model_presets[preset_name]

        info_md = f"""
### {preset_name}

**èª¬æ˜**: {preset['description']}
**æ¨å¥¨ç”¨é€”**: {preset['recommended_for']}
**Repository ID**: `{preset['repo_id']}`
"""

        return info_md, preset['repo_id'], preset['local_name']

    def save_last_model_path(self, model_path: str):
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆsettingså«ã‚€ï¼‰"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.last_model_cache_file.exists():
                try:
                    data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æ›´æ–°
            data["last_model"] = model_path

            self.last_model_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def save_inference_settings(self, temperature: float, max_tokens: int, top_p: float):
        """æ¨è«–è¨­å®šã‚’ä¿å­˜"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.last_model_cache_file.exists():
                try:
                    data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # è¨­å®šã‚’æ›´æ–°
            data["inference_settings"] = {
                "temperature": temperature,
                "max_tokens": int(max_tokens),
                "top_p": top_p
            }

            self.last_model_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_last_model_path(self) -> Optional[str]:
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.last_model_cache_file.exists():
                data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                return data.get("last_model")
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

    def load_inference_settings(self) -> dict:
        """æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.last_model_cache_file.exists():
                data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                return data.get("inference_settings", {})
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

    def download_model(self, repo_id: str, local_name: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if not repo_id:
            return "ã‚¨ãƒ©ãƒ¼: Repository IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

        try:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            downloaded_path = self.model_manager.download_model(
                repo_id=repo_id,
                local_name=local_name if local_name else None
            )

            return f"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\nã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}"
